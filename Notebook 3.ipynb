{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c00084e",
   "metadata": {},
   "source": [
    "# Quantitative evaluation. Test your findings and make them pretty. A bit of statistics and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729955fe",
   "metadata": {},
   "source": [
    "For this lesson we want to visualize some results and encourage testing your findings before drawing conclusion.\n",
    "\n",
    "We'll first import some libraries and load our data as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_trump = []\n",
    "\n",
    "for speech in os.scandir(r\"C:\\Users\\au576018\\OneDrive - Aarhus Universitet\\Documents\\Kurser\\kvantitativ diskursanalyse\\quantitative_discourse_analysis\\data\\politicians\\english\\trump\"):\n",
    "    x = open(speech, encoding = \"utf8\")\n",
    "    texts_trump.append(x.read())\n",
    "    x.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25bfda",
   "metadata": {},
   "source": [
    "We want to work with both texts from Obama and Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a3baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_obama = []\n",
    "\n",
    "for speech in os.scandir(r\"C:\\Users\\au576018\\OneDrive - Aarhus Universitet\\Documents\\Kurser\\kvantitativ diskursanalyse\\quantitative_discourse_analysis\\data\\politicians\\english\\obama\"):\n",
    "    x = open(speech, encoding = \"utf8\")\n",
    "    texts_obama.append(x.read())\n",
    "    x.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101d321",
   "metadata": {},
   "source": [
    "We inspect our data. It is to alike groups of text, with 10 speeches from each speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfea0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts_obama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0aa87",
   "metadata": {},
   "source": [
    "Now we arrange our data in a dataframe. We use the pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505016a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7022e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches = DataFrame({\"Speeches\": texts_trump + texts_obama}) # make a dataframe from our two lists of text\n",
    "df_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaking = [\"Trump\"]*10 + [\"Obama\"]*10 # we add metadata that gives information of the speaker for each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches[\"Speaker\"] = speaking # this is added as a new column in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ea062",
   "metadata": {},
   "source": [
    "Now we clean our text by removing punctuations, and because we want to make an analysis of the mention of countries in the text, I make countries containing more that one word recognizable as one by adding an underscore at every space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wrd(text_a):\n",
    "    text_b = text_a.replace(\"\\n\",\" \")\n",
    "    text_c = text_b.replace(\"The United States of America\",\"The_United_States_of_America\")\n",
    "    text_0 = text_c.replace(\"The US\",\"The_US\")\n",
    "    text_1 = text_0.replace(\"United States\",\"United_States\")\n",
    "    text_2 = text_1.replace(\".\",\" \")\n",
    "    text_3 = text_2.replace(\",\",\" \")\n",
    "    text_4 = text_3.replace(\":\",\" \")\n",
    "    text_5 = text_4.replace(\"*\",\" \")\n",
    "    text_6 = text_5.replace(\"–\",\" \")\n",
    "    text_7 = text_6.replace(\"'\",\" \")\n",
    "    text_8 = text_7.replace(\"”\",\" \")\n",
    "    text_clean = text_8.replace(\"-\",\" \")\n",
    "    text_token = text_clean.split()\n",
    "    return text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dfa07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches[\"Clean_texts\"] = df_speeches.Speeches.apply(clean_wrd) # the clean text is added as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0097c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d9251",
   "metadata": {},
   "source": [
    "# How many mentions of countries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb16b9a",
   "metadata": {},
   "source": [
    "In this analysis we want to count how many times the United States is mentioned compared to how many time another country is mentioned in the speeches. In order to do so I have prepared a list containing every contry. This is loaded as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open(r\"C:\\Users\\au576018\\OneDrive - Aarhus Universitet\\Documents\\Kurser\\kvantitativ diskursanalyse\\quantitative_discourse_analysis\\countries.txt\", encoding=\"utf8\").read()\n",
    "countries = txt.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccefad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d15bb",
   "metadata": {},
   "source": [
    "I inspect the list of countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d2aeee",
   "metadata": {},
   "source": [
    "Since we don't want to count the United States both in the group of countings of the world's countries and in the countings of the United States, we remove it from the list of countries in the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ed193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"United States\" in countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.remove(\"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1615c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"United States\" in countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a6627",
   "metadata": {},
   "source": [
    "By close reading the speeches, we find mentions of the United States in several forms. We list these unlike forms in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd16200",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_list = [\"The_United_States_of_America\", \"United_States\", \"America\", \"The_US\", \"US\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a628cce",
   "metadata": {},
   "source": [
    "Now we define a function that counts how many times specific words from a list are mentioned in a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5208751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(txt, stopwords): # takes inputs: a text you want to count, and a list with the words you are counting\n",
    "    \n",
    "    count = 0\n",
    "    index = 0  # Starting index\n",
    "    \n",
    "    while index < len(txt):\n",
    "        word = txt[index]\n",
    "        \n",
    "        if word in stopwords:\n",
    "            a = stopwords.count(word)\n",
    "            count = count + a\n",
    "            # Remove the word from the list\n",
    "            txt.remove(word)\n",
    "        else:\n",
    "            index += 1  # Move to the next word if it's not a stopword\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c66fd",
   "metadata": {},
   "source": [
    "Let's try out the function with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f855151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "word_list = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\", \"is\", \"the\", \"of\", \"apple\", \"banana\"]\n",
    "stopwords = [\"is\", \"the\", \"of\"]\n",
    "\n",
    "counts = count_words(word_list, stopwords)\n",
    "print(\"Word Counts:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e9997",
   "metadata": {},
   "source": [
    "We now count every mentions of the words related to the United states and add the count as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches[\"US_counts\"] = [count_words(txt, us_list) for txt in df_speeches[\"Clean_texts\"].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dad4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07efdaa",
   "metadata": {},
   "source": [
    "We count every mentions of the countries as well and add the count as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf72bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches[\"World_counts\"] = [count_words(txt, countries) for txt in df_speeches[\"Clean_texts\"].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cca903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f87cd7",
   "metadata": {},
   "source": [
    "# Plot the frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e365a",
   "metadata": {},
   "source": [
    "These mentions gives an impression of whether the US or the world is mentioned more or less than another. This is numerical data, and a visualization could give a better understanding on how/if the countings are alike."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e308ca",
   "metadata": {},
   "source": [
    "For visualizing the data we need a library called 'matplotlib'.\n",
    "\n",
    "For more information about the package and for more ideas on different plot types, see the documentation: https://matplotlib.org/stable/plot_types/basic/bar.html#sphx-glr-plot-types-basic-bar-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # imports the library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638e06f",
   "metadata": {},
   "source": [
    "We now plot our data as a bar plot with the axis Speaker at the horisontal axis and the counts of mentions of the US at our vertical axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches.plot(x='Speaker', y='US_counts', kind='bar', legend=None)\n",
    "plt.title(\"Mentions of US\") # the title of the plot\n",
    "plt.xlabel('Speakers') # the name of the x axis\n",
    "plt.ylabel('Mentions') # the name of the y axis\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c52c9",
   "metadata": {},
   "source": [
    "We make the same type of plot for the counts of the mentions of countries in the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches.plot(x='Speaker', y='World_counts', kind='bar', legend=None)\n",
    "plt.title(\"Mentions of the World\") # the title of the plot \n",
    "plt.xlabel('Speakers') # the name of the x axis\n",
    "plt.ylabel('Mentions') # the name of the y axis\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c7e2e",
   "metadata": {},
   "source": [
    "To make it more readable you might want to plot your results together. But be careful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting specific columns to plot\n",
    "columns_to_plot = ['US_counts', 'World_counts']\n",
    "df_speeches[columns_to_plot].plot(kind='bar')\n",
    "plt.title('Mentions of US and the world')\n",
    "plt.xlabel('Speaker')\n",
    "plt.ylabel('Mentions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd8eeb",
   "metadata": {},
   "source": [
    "Small exercise: \n",
    "    \n",
    "- How do we read this plot?\n",
    "- What does it tell us about the speakers individually? Compared to each other?\n",
    "- Is it a good visualization?\n",
    "- How could we improve?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378084d",
   "metadata": {},
   "source": [
    "We make the following function in order to make a better evalutation of our findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c064350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(us_count, world_count): # takes two lists of numbers as input\n",
    "    us_counts = [int(number) for number in us_count] # make sure our datatype is integers\n",
    "    world_counts = [int(number) for number in world_count]\n",
    "    ratio = [] # space holder for the results of calculated ratios\n",
    "    for i in range(0,len(us_counts)): # for every element from index 0 to the length of the list \"us_count\"\n",
    "        ratio.append(us_counts[i]/(world_counts[i]+us_counts[i])) # calculate the ratio and append on ratio list\n",
    "    return ratio #return list with ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c494426",
   "metadata": {},
   "source": [
    "When using the function we now get a list of numbers between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b692b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio(df_speeches[\"US_counts\"].to_list(), df_speeches[\"World_counts\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d775d4",
   "metadata": {},
   "source": [
    "The ratios are added as columns in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be4463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches[\"Ratio_us_mentions\"] = ratio(df_speeches[\"US_counts\"].to_list(), df_speeches[\"World_counts\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea303c",
   "metadata": {},
   "source": [
    "By plotting the ratio it is more straight forward to compare Obama and Trumps mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches.plot(x='Speaker', y='Ratio_us_mentions', kind='bar', legend=None)\n",
    "plt.title(\"Ratio of US mentions compared to mentions of the world\")\n",
    "plt.xlabel('Speakers')\n",
    "plt.ylabel('Ratio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e39ea",
   "metadata": {},
   "source": [
    "A plot is an easy way to \"read\" data. Although it might be hard to conclude directly from a plot. It is a good idea to make some kind of statistic test that might strengthen your conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6972ddc",
   "metadata": {},
   "source": [
    "# Permutation test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ad595",
   "metadata": {},
   "source": [
    "When working with text one should always be careful on which statistical tests to use. The permutation test is great when working with two groups of data and wanting to test whether there is a difference from the one group to another. There is also no assumptions on the distribution of your data.\n",
    "The only thing to keep in mind is that one should have 30 data point or more in order to get useful results. We only work with 20 data points (a calculation for each test). We will go through with this, and discuss what to do about it afterwards.\n",
    "\n",
    "You can read more about the permutation test here: https://www.jwilber.me/permutationtest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69811e65",
   "metadata": {},
   "source": [
    "We want to use the permutation test to see whether there is a difference from the two groups in the ratio of mentions of the US compared to mentions of the world.\n",
    "\n",
    "Our null hypothesis is that here is no difference between the two groups, while the alternative hypothesis is that Trump has a bigger ratio of US mentions than Obama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc153f5a",
   "metadata": {},
   "source": [
    "We split the data in the two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40176726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump  = df_speeches[df_speeches[\"Speaker\"] == \"Trump\"] # makes a subset of the rows where the column \"Speaker\" is \"Trump\"\n",
    "df_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obama = df_speeches[df_speeches[\"Speaker\"] == \"Obama\"] # makes a subset of the rows where the column \"Speaker\" is \"Obama\"\n",
    "df_obama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d4dd7",
   "metadata": {},
   "source": [
    "We now need to use the following package to calculate the mean and make samples etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6761a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st # importing statistics library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ffae8",
   "metadata": {},
   "source": [
    "We now calculate the mean in ratio for both Trump and Obama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_us_ratio = st.mean(df_trump[\"Ratio_us_mentions\"].to_list()) # calculate the mean from the column \"Ratio_us_mentions\"\n",
    "trump_us_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a068c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_us_ratio = st.mean(df_obama[\"Ratio_us_mentions\"].to_list()) # calculate the mean from the column \"Ratio_us_mentions\"\n",
    "obama_us_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb67cc",
   "metadata": {},
   "source": [
    "We are now ready to make the permutation test of the mentions of US in the two groups. \n",
    "\n",
    "We will use a p-value = 0.05. This is the standard p-value, and you can always use this with no further explanation if you do not have a specific argument to choose another p-value. \n",
    "\n",
    "In words, what the permutation test calculates:\n",
    "\n",
    "- calculate the observed difference in means between the two groups (Trump and Obama)\n",
    "- make a list of all data points together (all ratios from both Trump and Obama)\n",
    "- sets N to be a big number. This is how many times you wants to permute (shuffle) your data\n",
    "- creates a list \"result\" to store your results from each permutation of your data\n",
    "\n",
    "The for-loop is the permutation\n",
    "- for indexes in range 0 to N (which was a very big number!) do the following:\n",
    "- index1: make 10 random indexes from range 0 to 20 (because we have 10 data points in one group and 20 data points in total)\n",
    "- index2: make a list of indexes which are not in use index1 (goes through all numbers op to 20, and store them as index if they aren't already on the list index1)\n",
    "- group1: stores the datapoints from the list of all data points, if they have an index from index1\n",
    "- group2: stores the datapoints from the list of all data points, if they have an index from index2\n",
    "- calculate the difference in mean in the two new groups of data and stores on the list \"result\"\n",
    "\n",
    "then every value from result is plotted in a histogram, and the observed difference in mean is plotted as a red line.\n",
    "\n",
    "the p-value is calculated by counting how many of the results from the permutations are greater than the observed difference, and then calculate it is a ratio (so you get the percentage instead of a number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f522a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import numpy as np\n",
    "\n",
    "observed = trump_us_ratio-obama_us_ratio # the observed difference between trump and obama\n",
    "all_data = df_speeches[\"Ratio_us_mentions\"].to_list()\n",
    "\n",
    "N = 10**4-1 # how many times we want to permute our data\n",
    "result = [] # storage for our results\n",
    "for i in range(0,N):\n",
    "    index1 =  sample(list(range(0,20)), 10)\n",
    "    index2 = [m for m in list(range(0,20)) if m not in index1]\n",
    "    group1 = [all_data[k] for k in index1]\n",
    "    group2 = [all_data[l] for l in index2]\n",
    "    result.append(st.mean(group1)-st.mean(group2))\n",
    "\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    "plt.hist(result, bins = 40)\n",
    "plt.title(\"Permutation test over the difference in mentions of the US\")\n",
    "plt.axvline(observed, color = \"red\", linestyle = \"--\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "greater_or_equals = [rs for rs in result if rs >= observed]\n",
    "\n",
    "pvalue = (len(greater_or_equals))/(N+1)  # en-sidet test\n",
    "pvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f48b44",
   "metadata": {},
   "source": [
    "We now have a plot and a p-value.\n",
    "\n",
    "### Questions\n",
    "- How do we read the plot?\n",
    "- What does the p-value mean?\n",
    "- What can we conclude from the test?\n",
    "- Is this an obvious conclusion compared to the bar plots above?\n",
    "- Could we improve the readability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64b5bf9",
   "metadata": {},
   "source": [
    "# Another example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3759d",
   "metadata": {},
   "source": [
    "From a close reading I have found that Trump and Obama use various terms when speaking of immigrants. Some are neutral and some are more negative. It of course depends on the context, but it would be nice if I could make some kind of test to see whether my perception of their discourse is correct.\n",
    "\n",
    "First I split my data in words and lower cases, and then I count for negative and neutral words regarding the immigrant discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ffe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_low(text_0):\n",
    "    text_1 = text_0.replace(\"\\n\",\" \")\n",
    "    text_2 = text_1.replace(\".\",\" \")\n",
    "    text_3 = text_2.replace(\",\",\" \")\n",
    "    text_4 = text_3.replace(\":\",\" \")\n",
    "    text_5 = text_4.replace(\"*\",\" \")\n",
    "    text_6 = text_5.replace(\"–\",\" \")\n",
    "    text_7 = text_6.replace(\"'\",\" \")\n",
    "    text_8 = text_7.replace(\"”\",\" \")\n",
    "    text_clean = text_8.replace(\"-\",\" \")\n",
    "    text_lower = text_clean.lower()\n",
    "    text_token = text_lower.split()\n",
    "    return text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb974d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches[\"Low\"] = df_speeches.Speeches.apply(clean_low)\n",
    "df_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_speeches[\"Low\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e07e9",
   "metadata": {},
   "source": [
    "In the following I have listed some words in use when speaking of immigrants, and I have listed them in neutral and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_neutral = [\"immigrant\", \"immigrants\", \"refugee\", \"refugees\", \"migrant\", \"mexican\"]\n",
    "word_list_negative = [\"terrorist\", \"terrorists\", \"monsters\", \"aliens\", \"smugglers\", \"smuggler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches[\"Neutral\"] = [count_words(txt, word_list_neutral) for txt in df_speeches[\"Low\"].to_list()]\n",
    "df_speeches[\"Negative\"] = [count_words(txt, word_list_negative) for txt in df_speeches[\"Low\"].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f11726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25885824",
   "metadata": {},
   "source": [
    "I calculate the ratio of negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f74c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches[\"Ratio_negative\"] = ratio(df_speeches[\"Negative\"].to_list(), df_speeches[\"Neutral\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910851d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0b731",
   "metadata": {},
   "source": [
    "Now we want to perform a permutation test to see whether there is a difference between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump  = df_speeches[df_speeches[\"Speaker\"] == \"Trump\"] # makes a subset of the rows where the column \"Speaker\" is \"Trump\"\n",
    "df_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obama  = df_speeches[df_speeches[\"Speaker\"] == \"Obama\"] # makes a subset of the rows where the column \"Speaker\" is \"Trump\"\n",
    "df_obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_negative_ratio = st.mean(df_trump[\"Ratio_negative\"].to_list()) # calculate the mean from the column \"Ratio_us_mentions\"\n",
    "trump_negative_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_negative_ratio = st.mean(df_obama[\"Ratio_negative\"].to_list()) # calculate the mean from the column \"Ratio_us_mentions\"\n",
    "obama_negative_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d3e65",
   "metadata": {},
   "source": [
    "Permutation test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537076a0",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- What should our hypothesis be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import numpy as np\n",
    "\n",
    "observed = trump_negative_ratio-obama_negative_ratio # the observed difference between trump and obama\n",
    "all_data = df_speeches[\"Ratio_negative\"].to_list()\n",
    "\n",
    "N = 10**4-1 # how many times we want to permute our data\n",
    "result = [] # storage for our results\n",
    "for i in range(0,N):\n",
    "    index1 =  sample(list(range(0,20)), 10)\n",
    "    index2 = [m for m in list(range(0,20)) if m not in index1]\n",
    "    group1 = [all_data[k] for k in index1]\n",
    "    group2 = [all_data[l] for l in index2]\n",
    "    result.append(st.mean(group1)-st.mean(group2))\n",
    "\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    "plt.hist(result, bins = 40)\n",
    "plt.title(\"Permutation test over the difference in ratio of negative words for immigrants\")\n",
    "plt.axvline(observed, color = \"red\", linestyle = \"--\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "greater_or_equals = [rs for rs in result if rs >= observed]\n",
    "\n",
    "pvalue = (len(greater_or_equals))/(N+1)  # en-sidet test\n",
    "pvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacec3c",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- HOw do we read the plot?\n",
    "- What should our conclusion be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a80a0",
   "metadata": {},
   "source": [
    "# Comment on the amount of data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44544603",
   "metadata": {},
   "source": [
    "Almost every statistical test requires at least 30 data points in order to be strong and have a high enough precision. An outlier would for example have a great impact on the result in the above data set, as it would not be as problematic in da data set with 100 or 1000 data points.\n",
    "\n",
    "If you work with for example interviews or are collection data, you can design your project to have at least 30 data points / 30 respondents to avoid the problem of having to little data.\n",
    "If you work with longer texts, for example novelles and books, you can always consider splitting the texts into chapters, sections etc. If you have no chapters or sections, you could choose an amount of words or sentences and divide your text into smaller parts from this. \n",
    "\n",
    "Depending on what you seek in your analysis, small amounts of data might give you other problems in order to test your data at all. Even if you have 30 texts, the counts may include a lot of zero values which could cause problems concluding anything. This should be a consideration in operationalizing - are you sure you are measuring the concept that you want to analyse?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
