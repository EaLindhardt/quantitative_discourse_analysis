{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa98dbe",
   "metadata": {},
   "source": [
    "# How to 'find stuff' your text. Use Spacy as a tool for discourse analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c880c",
   "metadata": {},
   "source": [
    "We start by importing necessary libraries and import our data to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74affca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "text = open(r\"C:\\Users\\au576018\\OneDrive - Aarhus Universitet\\Documents\\Kurser\\kvantitativ diskursanalyse\\quantitative_discourse_analysis\\data\\politicians\\english\\trump\\trump_03_01_2020.txt\", encoding=\"utf8\")\n",
    "speech = text.read()\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech # Display the raw speech text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(speech) # Display the speech with formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb31d4",
   "metadata": {},
   "source": [
    "To get rid of \"disturbing elements\" in the raw text, such as \"\\n\", we clean up the speech a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f25647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the speech text by replacing newline characters with spaces\n",
    "\n",
    "speech_clean = speech.replace(\"\\n\", \" \")\n",
    "speech_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310acb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(speech_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7563ce",
   "metadata": {},
   "source": [
    "We want to use the library Spacy for performing our discourse analysis\n",
    "\n",
    "For guides and documentation for spacy, see the following: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245de9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # this imports the spacy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90adbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\") # specify which language model you wants to use. This is the large english model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a3d66",
   "metadata": {},
   "source": [
    "Now we want to transform our text file into a nlp-element. We use our language model to do that.\n",
    "By doing this, metadata such as part of speech tags, sentences, morphological information etc. is added to every word. We can use 'calls' (code commands) to retrieve this information when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(speech_clean) # save text as nlp-element under the variable name 'doc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(speech_clean) # Display the type of the cleaned speech text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc) # Display the type of the Spacy document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad084d8",
   "metadata": {},
   "source": [
    "We can now use the functionality from spacy in our analysis of the document. For example we can go through each sentence one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(doc.sents)) # evey sentences in a list. The sentences are seperated by a comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c982cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sents: # Print sentences one by one\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46152ca7",
   "metadata": {},
   "source": [
    "# Keyword analysis\n",
    "In \"Acceptable Bias? Using corpus linguistic methods with critical discourse analysis\" by Paul Baker, he uses a \"key word\" analysis. In order to choose appropiate keywords for your analysis, you could base it on your knowledge from within your field, from close reading analysis' etc. Another apporach could be to let your data decide for you (this would be an explorative approach). Is some words more frequent than others? Are there specific patterns?\n",
    "\n",
    "In the following we will find the most frequent words within the speech and base our analysis on those words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282ab52",
   "metadata": {},
   "source": [
    "We use the in-built functionality that labes every word with their part of speech-tag:\n",
    "\n",
    "Alphabetical listing\n",
    "\n",
    "- ADJ: adjective\n",
    "- ADP: adposition\n",
    "- ADV: adverb\n",
    "- AUX: auxiliary\n",
    "- CCONJ: coordinating conjunction\n",
    "- DET: determiner\n",
    "- INTJ: interjection\n",
    "- NOUN: noun\n",
    "- NUM: numeral\n",
    "- PART: particle\n",
    "- PRON: pronoun\n",
    "- PROPN: proper noun\n",
    "- PUNCT: punctuation\n",
    "- SCONJ: subordinating conjunction\n",
    "- SYM: symbol\n",
    "- VERB: verb\n",
    "- X: other\n",
    "\n",
    "© 2014–2022 Universal Dependencies contributors. Site powered by Annodoc and brat. https://universaldependencies.org/u/pos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tokens with their part-of-speech tags\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481b261",
   "metadata": {},
   "source": [
    "In order to look at relevant words in the text, we are interested in the nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [token.text for token in doc if token.pos_ == \"NOUN\"] # Extract nouns from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns # prints list of nouns within the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639527d4",
   "metadata": {},
   "source": [
    "in order to identify alike words as the same word, even though they might appear in unlike forms, we lemmatize our words.\n",
    "Lemmatization involves reducing words to their base or root form, which helps in grouping different inflected forms of a word together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dbc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_nouns = [token.lemma_ for token in doc if token.pos_ == \"NOUN\"] # Extract lemmatized nouns from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(lemma_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(lemma_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a73ca",
   "metadata": {},
   "source": [
    "# The most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5df2b",
   "metadata": {},
   "source": [
    "When we are working with one text it is pretty straight forward to count the most frequent word. But when working with several texts you should use a counter function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define a function that counts the frequence of every word in a list of words, and returns every word \n",
    "# with its number of appearances, sorted from the most to the least frequent word\n",
    "\n",
    "from collections import Counter #imports a counting-function from the library 'collections'\n",
    "def sorted_count(list_of_str): # defining the name of the function 'sorted_count' and the input \"list_of_str\"\n",
    "    b = Counter(list_of_str) # counts the frequency of every word on the list_of_str, and assign it to the value b\n",
    "    c = sorted(b.items(), key=lambda item: item[1], reverse = True) # sort the elements assigned to value b and assign to c\n",
    "    return c # returns the sorted counts 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa392be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_count(lemma_nouns) # here we call the function on our list of strings \"lemma_nouns\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a6db7",
   "metadata": {},
   "source": [
    "We see the five most frequent words are: \n",
    "    \n",
    "```\n",
    "('people', 6),\n",
    "('world', 5),\n",
    "('nation', 3),\n",
    "('military', 3),\n",
    "()'leadership', 3)\n",
    "\n",
    "```\n",
    "\n",
    "We want to use \"people\" as our keyword in the following analysis.\n",
    "\n",
    "In what context does 'people' appear? \n",
    "Let's first find the sentences containing the word, in order to make a close reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b542cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find sentences containing a specific keyword\n",
    "\n",
    "def sentence_with_word(nlp_object, keyword): # name of function is \"sentence with word\", and takes a nlp-element and a the keyword as an input\n",
    "    sentences = [sentence for sentence in nlp_object.sents] # makes a list with all the sentences in the text\n",
    "    with_keyword = [] # makes an empty list as placeholder for all the sentences containing the keyword\n",
    "    for sentence in sentences: # looks in every sentence on the list of sentences\n",
    "        tokens = [str(token) for token in sentence] # makes a list of every word in the sentence\n",
    "        if keyword in tokens: # checks whether the keyword is in the list of words from the sentence\n",
    "            with_keyword.append(sentence) # if yes, apply the whole sentence to the list \"with_keyword\"\n",
    "    if len(with_keyword) > 0: # checks whether we have any sentences in the list of sentences with the keyword\n",
    "        return with_keyword # if yes, print the list of sentences with the keyword\n",
    "    else: # if the list is empty\n",
    "        return \"Keyword not in text\" # print this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47eb35a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_with_word(doc, \"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63061f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_people = sentence_with_word(doc, \"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ffba8",
   "metadata": {},
   "source": [
    "We can read four ourselves the associated adjectives, when we are working with a single textfile. But we want to make a program that finds the associated adjectives, in order to use it when working with several text files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6227bc",
   "metadata": {},
   "source": [
    "# Simple example on extracting adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"We have green apples and sour lemons given with great love from the King of the seas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = nlp(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6be917",
   "metadata": {},
   "source": [
    "We want to find the associated adjectives, and use 'noun chunks' in order to do so. Noun chunks are “base noun phrases” – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, “the lavish green grass” or “the world’s largest tech fund”. To get the noun chunks in a document, simply iterate over Doc.noun_chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039723e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for chunk in doc_test.noun_chunks: # find noun chunks\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some other features to get from the chunks\n",
    "for chunk in doc_test.noun_chunks:\n",
    "    print(chunk.text, \"\\n\", \"Text of root:\", chunk.root.text, \"\\n\", \"Dependency tag:\", chunk.root.dep_, \"\\n\",\n",
    "          \"Root in sentence:\", chunk.root.head.text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb90f0",
   "metadata": {},
   "source": [
    "please read the documentation, if you wants to dig deeper into the noun chunks: https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64aeea",
   "metadata": {},
   "source": [
    "we now want to go through each noun chunk in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8999c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc_test.noun_chunks: # for every chunk en the chunks of the nlp_document\n",
    "    for token in chunk: # for every word in the chunk\n",
    "        print(token.text, token.pos_) # #print the text of the word and the part of speech tag of the word\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45807a0a",
   "metadata": {},
   "source": [
    "and extract the adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf93465",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = [] # makes an empty list as a placeholder for the adjectives\n",
    "\n",
    "for chunk in doc_test.noun_chunks: # for every chunk en the chunks of the nlp_document\n",
    "    for token in chunk: # for every word in the chunk\n",
    "        if token.pos_ == \"ADJ\": # if the part of speech tag of the word is \"ADJ\"\n",
    "            adjectives.append(token.text) # append the word to the list \"adjectives\"\n",
    "            \n",
    "adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa384fa",
   "metadata": {},
   "source": [
    "# Let's apply it in our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c790f5b",
   "metadata": {},
   "source": [
    "Now we want to find the adjectives related to \"people\" in the original speech by Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in with_people: # for every sentence in the list of sentences\n",
    "    for chunk in sentence.noun_chunks: # for every noun chunk in the sentences\n",
    "        print(chunk.text) # print the noun chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36dc338",
   "metadata": {},
   "source": [
    "but we only wanted the chunks describing the word \"people\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a40eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_with_people = [] # makes an empty list as a placeholder for the noun chunks\n",
    "\n",
    "for sentence in with_people: # for every sentence in the list of sentences\n",
    "    for chunk in sentence.noun_chunks: # for every noun chunk in the sentences\n",
    "        if str(chunk.root) == \"people\": # if the root of the chunk is \"people\"\n",
    "            chunks_with_people.append(chunk) # append the noun chunk to the list \"chunks_with_people\"\n",
    "            \n",
    "chunks_with_people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69241030",
   "metadata": {},
   "source": [
    "now we can extract the adjectives from these noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fab8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adjectives = [] # makes an empty list as a placeholder for the adjectives\n",
    "\n",
    "for chunk in chunks_with_people: # for every sentence in the list of chunks_with_people\n",
    "    for token in chunk: # for every word in the chunk\n",
    "        if token.pos_ == \"ADJ\": # if the part of speech tag is \"ADJ\"\n",
    "            adjectives.append(token.text) # append the word to the list \"adjectives\"\n",
    "            \n",
    "adjectives # print the list \"adjectives\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bb1ca",
   "metadata": {},
   "source": [
    "# Exercise: Make the same analysis for another frequent word in the speech\n",
    "\n",
    "Help:\n",
    "- start by finding the senteces which include your word\n",
    "- find the noun chunks in the sentences which include your word\n",
    "- find the specific noun chunks that describe your word\n",
    "\n",
    "\n",
    "(SOLUTION is further down. But try your best and ask for help before looking at the solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067306a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de27cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145193a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87467bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bf68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ca903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca4732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef36cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe7b5934",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03008441",
   "metadata": {},
   "source": [
    "We remember the five most frequent words:\n",
    "\n",
    "```\n",
    "('people', 6),\n",
    "('world', 5),\n",
    "('nation', 3),\n",
    "('military', 3),\n",
    "('leadership', 3)\n",
    "```\n",
    "\n",
    "I now want to use \"military\" as our keyword in the following analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650bc5f8",
   "metadata": {},
   "source": [
    "I use my function from earlier to find the sentences that include the word \"military\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28229e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_with_word(doc, \"military\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76d2e9",
   "metadata": {},
   "source": [
    "I save these sentences by assigning it to a variable \"with_military\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_military = sentence_with_word(doc, \"military\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ecf766",
   "metadata": {},
   "source": [
    "I now find the nouns chunks by using the code from above amd change the word \"people\" with \"military\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b66846",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_with_military = [] # makes an empty list as a placeholder for the noun chunks\n",
    "\n",
    "for sentence in with_military: # for every sentence in the list of sentences\n",
    "    for chunk in sentence.noun_chunks: # for every noun chunk in the sentences\n",
    "        if str(chunk.root) == \"military\": # if the root of the chunk is \"people\"\n",
    "            chunks_with_military.append(chunk) # append the noun chunk to the list \"chunks_with_people\"\n",
    "            \n",
    "chunks_with_military"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4910216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
